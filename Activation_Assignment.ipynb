{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9312eff7",
   "metadata": {},
   "source": [
    "Кейс-задание: «Анализ функций активации: непрерывность, дифференцируемость и численное поведение»\n",
    "\n",
    "Цель задания\n",
    "\n",
    "Сформулировать и подтвердить (с помощью ручных рассуждений и простейших численных экспериментов) ключевые свойства трёх функций активации — ReLU, Leaky ReLU (α=0.01) и Softplus — которые понадобятся впоследствии при знакомстве с обучением нейросетей:\n",
    "\n",
    "1. Доказать формальные свойства (непрерывность и/или дифференцируемость) через ε–δ и односторонние производные.\n",
    "2. Провести простейшие численные эксперименты (несколько точек, ручные вычисления или короткие скрипты на Python) для иллюстрации «негладких» точек и сравнения значений производных.\n",
    "3. Сделать графические зарисовки (ручные эскизы или с использованием matplotlib) таких функций и их «приближённых производных» (через конечные разности), чтобы показать, где и как меняется поведение.\n",
    "\n",
    "Без запуска «сложных» моделей — только ручные вычисления и короткие фрагменты кода, достаточные для нескольких численных примеров.\n",
    "\n",
    "Структура задания\n",
    "\n",
    "1. Теоретическая часть (40 %)\n",
    "\n",
    "1.1 ReLU:\n",
    "\n",
    "- Определение:\n",
    "  ReLU(z) = max(0, z).\n",
    "\n",
    "- Непрерывность:\n",
    "  • Докажите (через ε–δ или через проверку пределов), что ReLU непрерывна при любом z ∈ ℝ.\n",
    "\n",
    "- Дифференцируемость:\n",
    "  • Найдите односторонние производные в точке z = 0:\n",
    "    limₕ→0⁻ (ReLU(0 + h) - ReLU(0)) / h,\n",
    "    limₕ→0⁺ (ReLU(0 + h) - ReLU(0)) / h.\n",
    "  • Сделайте вывод о том, существует ли производная в z = 0.\n",
    "\n",
    "- Рисунок:\n",
    "  • Нарисуйте (от руки или в matplotlib) график функции ReLU на отрезке [−2, 2].\n",
    "  • На том же рисунке проведите (штриховой линией) «приближенную производную» ReLU, вычисленную в трех вручную выбранных точках z = −1.0, 0.0, 1.0 с помощью формулы конечных разностей f'(z) ≈ (f(z + ε) − f(z − ε)) / (2ε), ε = 0.001.\n",
    "    – Покажите, что для z > 0 значение ≈ 1, для z < 0 ≈ 0, а при z = 0 численно «разброс». \n",
    "\n",
    "1.2 Leaky ReLU (α = 0.01):\n",
    "\n",
    "- Определение:\n",
    "  ℓReLU(z) = { z, если z ≥ 0; 0.01·z, если z < 0 }.\n",
    "\n",
    "- Непрерывность:\n",
    "  • Покажите (формально или по смыслу), что Leaky ReLU непрерывна при всех z.\n",
    "\n",
    "- Дифференцируемость:\n",
    "  • Найдите односторонние производные в z = 0:\n",
    "    limₕ→0⁻ (ℓReLU(0 + h) − ℓReLU(0)) / h,\n",
    "    limₕ→0⁺ (ℓReLU(0 + h) − ℓReLU(0)) / h.\n",
    "  • Опишите «ступеньку» между ними и объясните, почему это все равно лучше, чем ReLU.\n",
    "\n",
    "- Рисунок + численный пример:\n",
    "  • Нарисуйте ℓReLU(z) и приближенную производную (через конечные разности ε = 0.001) в трех точках: z = −1.0, 0.0, 1.0.\n",
    "  • Запишите в табличке реальные значения (ℓReLU(z ± ε) разностей), чтобы показать значения ≈ 0.01, промежуточное, 1.0.\n",
    "\n",
    "1.3 Softplus:\n",
    "\n",
    "- Определение:\n",
    "  Softplus(z) = ln(1 + eᶻ).\n",
    "\n",
    "- Непрерывность и гладкость:\n",
    "  • Докажите (через композицию непрерывных функций), что Softplus непрерывна и бесконечно дифференцируема.\n",
    "\n",
    "- Вывод производной:\n",
    "  • Выведите вручную формулу d/dz ln(1 + eᶻ) = eᶻ / (1 + eᶻ) = σ(z), где σ(z) — сигмоида.\n",
    "\n",
    "- Численные проверки:\n",
    "  • Посчитайте (вручную или коротким скриптом) Softplus(z) и σ(z) для z = −5, 0, 5 (например, Softplus(0) ≈ 0.693, σ(0) = 0.5; Softplus(5) ≈ 5.0067, σ(5) ≈ 0.9933; Softplus(−5) ≈ 0.0067, σ(−5) ≈ 0.0067).\n",
    "\n",
    "- График:\n",
    "  • Постройте в matplotlib (или нарисуйте от руки) Softplus и его производную (σ(z)) на отрезке [−5, 5].\n",
    "\n",
    "1.4 Сравнительный анализ (таблица):\n",
    "\n",
    "Соберите информацию о ReLU, Leaky ReLU и Softplus в одну таблицу, указав:\n",
    "  1. Непрерывность при всех z?\n",
    "  2. Дифференцируемость при всех z? Если нет, укажите точки разрыва.\n",
    "  3. Односторонние производные в проблемных точках (z = 0 для ReLU и Leaky ReLU).\n",
    "  4. Сложность вычисления f(z) и f'(z) (количество операций, качественный комментарий).\n",
    "  5. Основные «ловушки» (мертвые нейроны, затухание градиента, переполнение exp(z)).\n",
    "\n",
    "2. Простейшие численные эксперименты (30 %)\n",
    "\n",
    "2.1 Приближённая производная ReLU (3 точки):\n",
    "\n",
    "Напишите в Python (или запишите вручную) код:\n",
    "  def relu(z): return max(0.0, z)\n",
    "  def approx_derivative(f, z, eps=1e-3): return (f(z + eps) - f(z - eps)) / (2*eps)\n",
    "  for z0 in [-1.0, 0.0, 2.0]: print(f\"ReLU' approx at {z0} =\", approx_derivative(relu, z0))\n",
    "\n",
    "Сохраните вывод и поясните: для z = −1 ≈ 0; z = 2 ≈ 1; z = 0 — «разброс».\n",
    "\n",
    "2.2 Приближённая производная Leaky ReLU (3 точки):\n",
    "\n",
    "  def leaky_relu(z, α=0.01): return z if z >= 0 else α*z\n",
    "  for z0 in [-1.0, 0.0, 2.0]: print(f\"Leaky ReLU' approx at {z0} =\", approx_derivative(leaky_relu, z0))\n",
    "Запишите результаты и сравните с ожидаемыми (≈0.01, промежуточное, ≈1).\n",
    "\n",
    "2.3 Softplus и сигмоида:\n",
    "\n",
    "  import math\n",
    "  def softplus(z): return math.log(1 + math.exp(z))\n",
    "  def sigmoid(z): return 1 / (1 + math.exp(-z))\n",
    "  for z0 in [-5.0, 0.0, 5.0]:\n",
    "    sp = softplus(z0)\n",
    "    sp_deriv = (softplus(z0 + 1e-3) - softplus(z0 - 1e-3)) / 2e-3\n",
    "    sg = sigmoid(z0)\n",
    "    print(f\"z={z0}: Softplus={sp:.6f}, approx derivative={sp_deriv:.6f}, sigmoid={sg:.6f}\")\n",
    "\n",
    "Сравните приближённую деривативу и σ(z).\n",
    "\n",
    "2.4 Большие значения z и численная устойчивость:\n",
    "\n",
    "  print(softplus(50))  # ожидаем ≈ 50.0\n",
    "  print(sigmoid(50))   # ожидаем ≈ 1.0\n",
    "  print(sigmoid(-50))  # ожидаем ≈ 0.0 (примерно 1e-22)\n",
    "\n",
    "Поясните: когда exp(z) может переполниться и почему Softplus(50) ≈ 50.\n",
    "\n",
    "3. Задачи для самостоятельного решения (30 %)\n",
    "\n",
    "3.1 Постройте (в matplotlib или от руки):\n",
    "  – ReLU, Leaky ReLU и Softplus на [−3, 3].\n",
    "  – Приближённые производные через конечные разности (ε = 0.001) для тех же функций.\n",
    "Подпишите оси и поясните «неровности» и «скругления».\n",
    "\n",
    "3.2 Доказательство гладкости Softplus:\n",
    "  – Запишите рассуждение через композицию e^z → 1 + ⋅ → ln(⋅); все операции бесконечно дифференцируемы.\n",
    "  – Прокомментируйте: какие могут проблемы при численных вычислениях (например, exp(100), overflow)?\n",
    "\n",
    "3.3 Ручные расчёты (без GPT):\n",
    "  – Для z₁ = −0.5, z₂ = 0, z₃ = 0.5 найдите:\n",
    "    • ReLU(zᵢ), Leaky ReLU(zᵢ), Softplus(zᵢ) (с примерами шагов).\n",
    "    • Односторонние производные ReLU, Leaky ReLU в z = 0 (через пределы).\n",
    "    • Приближённые производные (ε = 0.001) для каждой функции и zᵢ.\n",
    "  – Сверьте с результатами скрипта и опишите расхождения。\n",
    "\n",
    "3.4 Текстовый абзац:\n",
    "  – Напишите 5–6 предложений о том, почему при обучении нейросетей выбор активации влияет на скорость и стабильность обучения. Ссылайтесь на ваши выводы и эксперименты。\n",
    "  – Приведите пример сценария，в котором ReLU «умирает» (когда градиент = 0) и объясните «на языке» приближённых производных.\n",
    "\n",
    "Оформление решения\n",
    "\n",
    "- Формат：Jupyter Notebook или Markdown-файл с кодом，выводами и графиками。\n",
    "- Структура：\n",
    "  1。Теоретическая часть (ReLU，Leaky ReLU，Softplus)。\n",
    "  2。Численные эксперименты (фрагменты кода и результаты)。\n",
    "  3。Графики (из matplotlib или от руки)。\n",
    "  4。Ручные вычисления (таблицы)。\n",
    "  5。Аналитический абзац。\n",
    "\n",
    "- Критерии оценки：\n",
    "  - Полнота формальных доказательств (25 %)\n",
    "  - Точность численных экспериментов (25 %)\n",
    "  - Качество графиков (20 %)\n",
    "  - Аналитический текст (15 %)\n",
    "  - Оформление и структура (15 %)\n",
    "\n",
    "Удачи!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
