Функция активации,Непрерывна при всех z?,Дифференцируема при всех z?,Точки разрыва/недифференцируемости,Односторонние производные в проблемных точках,Сложность вычисления f(z),Сложность вычисления f'(z),Основные ловушки
ReLU,Да,"Нет, в точке z=0",z = 0,f'(-0) = 0; f'(+0) = 1,1 сравнение + max → ~1 операция,1 сравнение → ~1 операция,Мёртвые нейроны (z<0 → нулевой градиент)
Leaky ReLU,Да,"Нет, в точке z=0",z = 0,"f'(-0) = α; f'(+0) = 1 (например, α = 0.01)",1 сравнение + умножение → ~2 операции,1 сравнение (и иногда умножение) → ~1–2 операции,"Меньше мёртвых нейронов, но маленький градиент при z<0; важен выбор α"
Softplus,Да,"Да, везде",Нет,Не применимо (гладкая функция),exp + сложение + логарифм → дорогая (~десятки операций),exp + сложение + деление → дорогая (~десятки операций),Затухание градиента при больших отрицательных z; переполнение exp при больших положительных z; медленные вычисления
